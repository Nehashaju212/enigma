![image](IMG-20240925-WA0030.jpg)

# JUST A VOICE - BUILDATHON

JUST A VOICE is an innovative application designed to break down communication barriers between mute and non-mute individuals. The app translates sign language into spoken words in real time, allowing seamless communication without the need for an interpreter. By using advanced machine learning algorithms, JUST A VOICE captures gestures through a webcam, processes them into text, and then converts the text into speech using a text-to-speech engine. This solution empowers individuals with speech disabilities to participate in conversations, presentations, and other daily interactions with ease, enhancing accessibility and inclusion in both personal and professional settings

## Team Members
1.NEHA SHAJU (Nehashaju212)   
2.SREELAKSHMI UV (SREELAKSHMIIUV)   
3.PRANAV BABU ()   
4.NIRMAL JOHN (nirmaljohn2004)   

## Link to Project
https://colab.research.google.com/drive/1jtji59fcBLxb1_fYPKmLNwVx7DEXYNF-?usp=sharing

## How it Works ?  
JUST A VOICE is an innovative application designed to translate sign language into spoken words, making communication easier between mute and non-mute individuals. Here’s how you can use it: 

Step-by-Step Guide: 

1. Set Up Your Environment 

Open the Application: Launch the GestureVoice application on your computer. 

Connect a Webcam: Ensure your webcam is connected and working properly, as it will capture your sign language gestures. 

2. Start the Application 

Begin Capturing: Click on the “Start” button to begin capturing video from your webcam. 

Position Yourself: Make sure you are in a well-lit area and position yourself so that your hands and face are clearly visible to the camera. 

3. Perform Sign Language Gestures 

Sign Naturally: Use your usual sign language gestures to communicate. The application will capture these gestures in real-time. 

Stay Within Frame: Ensure your hands stay within the camera frame for accurate detection. 

4. Real-Time Processing 

Gesture Recognition: The application uses advanced machine learning models to recognize your sign language gestures. 

Visual Feedback: You will see the recognized signs and corresponding text displayed on the screen. 

5. Audio Output 

Text-to-Speech Conversion: The recognized signs are converted into text and then into speech using a text-to-speech engine. 

Listen to the Output: The application will play the synthesized speech, allowing the audience to hear what you are signing. 

6. Interactive Communication 

Respond to Questions: If the audience asks questions, you can respond using sign language, and the application will translate your signs into spoken words. 

Adjust Settings: You can adjust settings such as volume, speech rate, and language preferences to suit your needs. 

Example Scenario: 

Imagine you are giving a presentation. You start JUST A VOICE, perform your sign language gestures, and the application translates them into spoken words that the audience can hear. This allows you to communicate effectively without needing an interpreter. 

Tips for Best Results: 

Good Lighting: Ensure you are in a well-lit area to help the camera capture your gestures clearly. 

Clear Background: Use a plain background to avoid distractions and improve gesture recognition accuracy. 

Practice: Spend some time practicing with the application to get comfortable with its features and ensure smooth communication. 



## Technologies used
1. Programming Languages 

Python: The primary language used for its extensive libraries and frameworks in machine learning and computer vision. 

2. Deep Learning Frameworks 

TensorFlow: An open-source library for machine learning and deep learning, used to build and train the gesture recognition model. 

Keras: An API built on top of TensorFlow, making it easier to construct and train neural networks. 

PyTorch: Another powerful deep learning framework known for its flexibility and ease of use, which can also be used for model development. 

3. Computer Vision Libraries 

OpenCV: Used for real-time computer vision tasks such as capturing video from a webcam and processing images. 

MediaPipe: A framework for building multimodal applied machine learning pipelines, particularly useful for extracting keypoints from hand, face, and body movements1. 

4. Model Architectures 

Convolutional Neural Networks (CNNs): Used for extracting spatial features from images. 

Long Short-Term Memory (LSTM): A type of recurrent neural network (RNN) used for handling sequential data and capturing temporal dependencies2. 

Gated Recurrent Units (GRUs): Another type of RNN similar to LSTMs but with a simpler architecture3. 

5. APIs and Services 

Text-to-Speech APIs: Services like Google Cloud Text-to-Speech or Azure Cognitive Services for converting text to audio output4. 

6. Development Tools 

Jupyter Notebooks: For interactive development and experimentation with machine learning models. 

Integrated Development Environments (IDEs): Such as PyCharm or Visual Studio Code for writing and debugging code. 

## Other Links
Provide any other links ( for eg. Wireframe , UI, Abstract, Presentation )
